{
  
    
        "post0": {
            "title": "Deep Convolutional Auto Encoders - Part 2",
            "content": "Introduction . In the first part I reimplemented the convolutional auto encoder from TimeCluster by Ali et al This time, I will adapt the model to handle all 300 flock agents. . In this notebook a 1D convolutional approach is evaluated . df = DataFrame(CSV.File(&quot;data_3.csv&quot;; header=false, types=Float32)) . 2,558 rows × 900 columns (omitted printing of 891 columns) . Column1Column2Column3Column4Column5Column6Column7Column8Column9 . Float32Float32Float32Float32Float32Float32Float32Float32Float32 . 1265.016 | -92.8037 | -38.0635 | 36.4608 | 117.116 | 248.638 | -293.46 | -105.428 | 83.4998 | . 2264.227 | -95.1109 | -41.1452 | 37.581 | 115.349 | 247.427 | -292.218 | -104.078 | 84.5678 | . 3263.411 | -97.4066 | -44.2264 | 38.6952 | 113.586 | 246.204 | -290.967 | -102.731 | 85.629 | . 4262.568 | -99.6902 | -47.3071 | 39.8027 | 111.823 | 244.975 | -289.706 | -101.388 | 86.6834 | . 5261.698 | -101.96 | -50.3869 | 40.9041 | 110.061 | 243.74 | -288.437 | -100.048 | 87.7312 | . 6260.808 | -104.212 | -53.4544 | 41.9991 | 108.298 | 242.5 | -287.158 | -98.7106 | 88.7723 | . 7259.9 | -106.444 | -56.5101 | 43.0754 | 106.529 | 241.251 | -285.871 | -97.3769 | 89.8069 | . 8258.973 | -108.655 | -59.5558 | 44.129 | 104.753 | 239.994 | -284.576 | -96.0464 | 90.8352 | . 9258.027 | -110.857 | -62.596 | 45.1645 | 102.969 | 238.734 | -283.272 | -94.719 | 91.8571 | . 10257.062 | -113.049 | -65.6334 | 46.1831 | 101.171 | 237.478 | -281.96 | -93.3949 | 92.8729 | . 11256.079 | -115.231 | -68.6708 | 47.185 | 99.3617 | 236.227 | -280.641 | -92.0738 | 93.8825 | . 12255.077 | -117.401 | -71.7104 | 48.1707 | 97.5404 | 234.979 | -279.314 | -90.7558 | 94.8861 | . 13254.057 | -119.559 | -74.7539 | 49.1499 | 95.7101 | 233.74 | -277.979 | -89.4409 | 95.8838 | . 14253.014 | -121.711 | -77.7902 | 50.1225 | 93.8716 | 232.508 | -276.637 | -88.129 | 96.8756 | . 15251.947 | -123.863 | -80.8264 | 51.0874 | 92.0255 | 231.28 | -275.288 | -86.8201 | 97.8617 | . 16250.857 | -126.01 | -83.8572 | 52.0437 | 90.1709 | 230.06 | -273.932 | -85.5141 | 98.8422 | . 17249.741 | -128.153 | -86.8853 | 52.9906 | 88.3083 | 228.843 | -272.569 | -84.2111 | 99.8172 | . 18248.596 | -130.298 | -89.9164 | 53.9274 | 86.4385 | 227.631 | -271.2 | -82.9109 | 100.787 | . 19247.423 | -132.437 | -92.9436 | 54.8581 | 84.567 | 226.416 | -269.824 | -81.6137 | 101.751 | . 20246.219 | -134.569 | -95.967 | 55.7864 | 82.6944 | 225.201 | -268.441 | -80.3192 | 102.71 | . 21244.979 | -136.694 | -98.9864 | 56.7131 | 80.8216 | 223.985 | -267.053 | -79.0275 | 103.663 | . 22243.699 | -138.811 | -102.001 | 57.6553 | 78.9576 | 222.767 | -265.659 | -77.7386 | 104.612 | . 23242.377 | -140.919 | -105.011 | 58.613 | 77.1027 | 221.548 | -264.258 | -76.4523 | 105.556 | . 24241.009 | -143.021 | -108.002 | 59.5861 | 75.2569 | 220.327 | -262.852 | -75.1688 | 106.494 | . 25239.597 | -145.115 | -110.973 | 60.5805 | 73.4226 | 219.105 | -261.441 | -73.8878 | 107.428 | . 26238.144 | -147.202 | -113.922 | 61.5957 | 71.6004 | 217.883 | -260.024 | -72.6095 | 108.358 | . 27236.651 | -149.28 | -116.851 | 62.6294 | 69.789 | 216.661 | -258.601 | -71.3337 | 109.282 | . 28235.12 | -151.349 | -119.757 | 63.6788 | 67.9873 | 215.437 | -257.174 | -70.0604 | 110.202 | . 29233.553 | -153.407 | -122.64 | 64.7414 | 66.194 | 214.212 | -255.741 | -68.7895 | 111.118 | . 30231.949 | -155.455 | -125.515 | 65.8158 | 64.4105 | 212.984 | -254.304 | -67.5211 | 112.029 | . &vellip;&vellip; | &vellip; | &vellip; | &vellip; | &vellip; | &vellip; | &vellip; | &vellip; | &vellip; | . Data Noramlisation and preperation . Normalise the data the same way as before; into the range [0, 1] as per the paper. . We then create a sliding window using the defaults from the paper where stride = 1 and window_size = 60 . Then we shuffle the data and split into train, validate and test subsets . function normalise(M) min = minimum(minimum(eachcol(M))) max = maximum(maximum(eachcol(M))) return (M .- min) ./ (max - min) end normalised = Array(df) |&gt; normalise window_size = 60 data = slidingwindow(normalised&#39;,window_size,stride=1) train, validate, test = splitobs(shuffleobs(data), (0.7,0.2)); . Define the encoder and decoder . We can define the network shape in a couple of different ways: . Keeping the convolution 1 dimentional and simply increasing the number of features from 3 to 900 (3 * num_of_agents) | Using 2D convolution: window_size X num_of_agents x dimensions (3) x batch | . In this notebook we will look at the 1D approach. . 1D Convolution . Adjusted the dimension expansion from ≈ 21x to 10x Also an aditional Conv/ConvTranspose step is added to reduce the dimensionality of the encoded space further . function create_ae_1d() # Define the encoder and decoder networks encoder = Chain( # 60x900xb Conv((9,), 900 =&gt; 9000, relu; pad = SamePad()), MaxPool((2,)), # 30x9000xb Conv((5,), 9000 =&gt; 4500, relu; pad = SamePad()), MaxPool((2,)), # 15x4500xb Conv((5,),4500 =&gt; 2250, relu; pad = SamePad()), # 15x2250xb MaxPool((3,)), Conv((3,),2250 =&gt; 1000, relu; pad = SamePad()), Conv((3,),1000 =&gt; 100, relu; pad = SamePad()), # 5x100xb Flux.flatten, Dense(500,100) ) decoder = Chain( Dense(100,500), (x -&gt; reshape(x, 5,100,:)), # 5x100xb ConvTranspose((3,), 100 =&gt; 1000, relu; pad = SamePad()), ConvTranspose((3,), 1000 =&gt; 2250, relu; pad = SamePad()), Upsample((3,)), # 15x2250xb ConvTranspose((5,), 2250 =&gt; 4500, relu; pad = SamePad()), Upsample((2,)), # 30x4500xb ConvTranspose((5,), 4500 =&gt; 9000, relu; pad = SamePad()), Upsample((2,)), # 60x9000xb ConvTranspose((9,), 9000 =&gt; 900, relu; pad = SamePad()), # 60x900xb ) return (encoder, decoder) end . create_ae_1d (generic function with 1 method) . Training . Training needs to be slightly adapted for each version of model we use. Also we now use train/validation/test sets for more accurate performance calculation. I&#39;ve also added learning rate adjustment and automatic model saving. . function save_model(m, epoch, loss) model_row = LegolasFlux.ModelRow(; weights = fetch_weights(cpu(m)),architecture_version=1, loss=0.0001) write_model_row(&quot;1d_300_model-$epoch-$loss.arrow&quot;, model_row) end function rearrange_1D(x) permutedims(cat(x..., dims=3), [2,1,3]) end function train_model_1D!(model, train, validate, opt; epochs=20, bs=16, dev=Flux.gpu) ps = Flux.params(model) local train_loss, train_loss_acc local validate_loss, validate_loss_acc local last_improvement = 0 local prev_best_loss = 0.01 local improvement_thresh = 5.0 validate_losses = Vector{Float64}() for e in 1:epochs train_loss_acc = 0.0 for x in eachbatch(train, size=bs) x = rearrange_1D(x) |&gt; dev gs = Flux.gradient(ps) do train_loss = Flux.Losses.mse(model(x),x) return train_loss end train_loss_acc += train_loss Flux.update!(opt, ps, gs) end validate_loss_acc = 0.0 for y in eachbatch(validate, size=bs) y = rearrange_1D(y) |&gt; dev validate_loss = Flux.Losses.mse(model(y), y) validate_loss_acc += validate_loss end validate_loss_acc = round(validate_loss_acc / (length(validate)/bs); digits=6) train_loss_acc = round(train_loss_acc / (length(train)/bs) ;digits=6) if validate_loss_acc &lt; 0.001 if validate_loss_acc &lt; prev_best_loss @info &quot;new best accuracy $validate_loss_acc saving model...&quot; save_model(model, e, validate_loss_acc) last_improvement = e prev_best_loss = validate_loss_acc elseif (e - last_improvement) &gt;= improvement_thresh &amp;&amp; opt.eta &gt; 1e-5 @info &quot;Not improved in $improvement_thresh epochs. Dropping learning rate to $(opt.eta / 2.0)&quot; opt.eta /= 2.0 last_improvement = e # give it some time to improve improvement_thresh = improvement_thresh * 1.5 elseif (e - last_improvement) &gt;= 15 @info &quot;Not improved in 15 epochs. Converged I guess&quot; break end end push!(validate_losses, validate_loss_acc) println(&quot;Epoch $e/$epochs t train loss: $train_loss_acc t validate loss: $validate_loss_acc&quot;) end validate_losses end . train_model_1D! (generic function with 1 method) . losses_0001 = train_model_1D!(model, train, validate, Flux.Optimise.ADAM(0.0001); epochs=200, bs=48); . ┌ Warning: The specified values for size and/or count will result in 21 unused data points └ @ MLDataPattern /opt/julia/packages/MLDataPattern/KlSmO/src/dataview.jl:205 . Epoch 1/200 train loss: 0.430836 validate loss: 0.057712 Epoch 2/200 train loss: 0.055009 validate loss: 0.051833 Epoch 3/200 train loss: 0.053515 validate loss: 0.051719 Epoch 4/200 train loss: 0.053483 validate loss: 0.051669 Epoch 5/200 train loss: 0.053481 validate loss: 0.051648 Epoch 6/200 train loss: 0.05348 validate loss: 0.051622 Epoch 7/200 train loss: 0.053478 validate loss: 0.051622 Epoch 8/200 train loss: 0.053471 validate loss: 0.051615 Epoch 9/200 train loss: 0.053464 validate loss: 0.051602 Epoch 10/200 train loss: 0.053461 validate loss: 0.051603 Epoch 11/200 train loss: 0.053447 validate loss: 0.051597 Epoch 12/200 train loss: 0.053427 validate loss: 0.051574 Epoch 13/200 train loss: 0.053222 validate loss: 0.050061 Epoch 14/200 train loss: 0.040074 validate loss: 0.033838 Epoch 15/200 train loss: 0.034933 validate loss: 0.033222 Epoch 16/200 train loss: 0.029958 validate loss: 0.018663 Epoch 17/200 train loss: 0.015721 validate loss: 0.012654 Epoch 18/200 train loss: 0.011821 validate loss: 0.010214 Epoch 19/200 train loss: 0.009049 validate loss: 0.007028 Epoch 20/200 train loss: 0.006481 validate loss: 0.005952 Epoch 21/200 train loss: 0.004894 validate loss: 0.003345 Epoch 22/200 train loss: 0.002088 validate loss: 0.001259 Epoch 23/200 train loss: 0.001 validate loss: 0.001243 . ┌ Info: new best accuracy 0.000762 saving model... └ @ Main In[9]:41 . Epoch 24/200 train loss: 0.000691 validate loss: 0.000762 . ┌ Info: new best accuracy 0.000597 saving model... └ @ Main In[9]:41 . Epoch 25/200 train loss: 0.000683 validate loss: 0.000597 . ┌ Info: new best accuracy 0.000309 saving model... └ @ Main In[9]:41 . Epoch 26/200 train loss: 0.000584 validate loss: 0.000309 . ┌ Info: new best accuracy 0.000142 saving model... └ @ Main In[9]:41 . Epoch 27/200 train loss: 0.000244 validate loss: 0.000142 . ┌ Info: new best accuracy 9.1e-5 saving model... └ @ Main In[9]:41 . Epoch 28/200 train loss: 0.000118 validate loss: 9.1e-5 . ┌ Info: new best accuracy 7.4e-5 saving model... └ @ Main In[9]:41 . Epoch 29/200 train loss: 8.6e-5 validate loss: 7.4e-5 . ┌ Info: new best accuracy 6.3e-5 saving model... └ @ Main In[9]:41 . Epoch 30/200 train loss: 7.1e-5 validate loss: 6.3e-5 Epoch 31/200 train loss: 6.5e-5 validate loss: 6.5e-5 . ┌ Info: new best accuracy 5.6e-5 saving model... └ @ Main In[9]:41 . Epoch 32/200 train loss: 6.3e-5 validate loss: 5.6e-5 . ┌ Info: new best accuracy 4.8e-5 saving model... └ @ Main In[9]:41 . Epoch 33/200 train loss: 5.3e-5 validate loss: 4.8e-5 . ┌ Info: new best accuracy 4.2e-5 saving model... └ @ Main In[9]:41 . Epoch 34/200 train loss: 4.6e-5 validate loss: 4.2e-5 . ┌ Info: new best accuracy 3.9e-5 saving model... └ @ Main In[9]:41 . Epoch 35/200 train loss: 4.2e-5 validate loss: 3.9e-5 . ┌ Info: new best accuracy 3.6e-5 saving model... └ @ Main In[9]:41 . Epoch 36/200 train loss: 3.8e-5 validate loss: 3.6e-5 . ┌ Info: new best accuracy 3.3e-5 saving model... └ @ Main In[9]:41 . Epoch 37/200 train loss: 3.5e-5 validate loss: 3.3e-5 . ┌ Info: new best accuracy 3.1e-5 saving model... └ @ Main In[9]:41 . Epoch 38/200 train loss: 3.3e-5 validate loss: 3.1e-5 . ┌ Info: new best accuracy 2.9e-5 saving model... └ @ Main In[9]:41 . Epoch 39/200 train loss: 3.1e-5 validate loss: 2.9e-5 . ┌ Info: new best accuracy 2.7e-5 saving model... └ @ Main In[9]:41 . Epoch 40/200 train loss: 2.9e-5 validate loss: 2.7e-5 Epoch 41/200 train loss: 2.7e-5 validate loss: 2.8e-5 . ┌ Info: new best accuracy 2.6e-5 saving model... └ @ Main In[9]:41 . Epoch 42/200 train loss: 2.7e-5 validate loss: 2.6e-5 Epoch 43/200 train loss: 3.0e-5 validate loss: 2.9e-5 Epoch 44/200 train loss: 6.2e-5 validate loss: 8.7e-5 Epoch 45/200 train loss: 0.000182 validate loss: 0.000175 Epoch 46/200 train loss: 0.000311 validate loss: 0.000224 Epoch 47/200 train loss: 0.00104 validate loss: 0.000602 . ┌ Info: Not improved in 5 epochs. Dropping learning rate to 5.0e-5 └ @ Main In[9]:46 . Epoch 48/200 train loss: 0.000221 validate loss: 5.7e-5 Epoch 49/200 train loss: 3.7e-5 validate loss: 2.6e-5 . ┌ Info: new best accuracy 2.4e-5 saving model... └ @ Main In[9]:41 . Epoch 50/200 train loss: 2.5e-5 validate loss: 2.4e-5 . ┌ Info: new best accuracy 2.2e-5 saving model... └ @ Main In[9]:41 . Epoch 51/200 train loss: 2.2e-5 validate loss: 2.2e-5 . ┌ Info: new best accuracy 2.1e-5 saving model... └ @ Main In[9]:41 . Epoch 52/200 train loss: 2.1e-5 validate loss: 2.1e-5 . ┌ Info: new best accuracy 2.0e-5 saving model... └ @ Main In[9]:41 . Epoch 53/200 train loss: 2.0e-5 validate loss: 2.0e-5 . ┌ Info: new best accuracy 1.9e-5 saving model... └ @ Main In[9]:41 . Epoch 54/200 train loss: 1.9e-5 validate loss: 1.9e-5 . ┌ Info: new best accuracy 1.8e-5 saving model... └ @ Main In[9]:41 . Epoch 55/200 train loss: 1.9e-5 validate loss: 1.8e-5 Epoch 56/200 train loss: 1.8e-5 validate loss: 1.8e-5 . ┌ Info: new best accuracy 1.7e-5 saving model... └ @ Main In[9]:41 . Epoch 57/200 train loss: 1.8e-5 validate loss: 1.7e-5 Epoch 58/200 train loss: 1.7e-5 validate loss: 1.7e-5 . ┌ Info: new best accuracy 1.6e-5 saving model... └ @ Main In[9]:41 . Epoch 59/200 train loss: 1.7e-5 validate loss: 1.6e-5 Epoch 60/200 train loss: 1.7e-5 validate loss: 1.6e-5 Epoch 61/200 train loss: 1.6e-5 validate loss: 1.6e-5 Epoch 62/200 train loss: 1.6e-5 validate loss: 1.6e-5 . ┌ Info: new best accuracy 1.5e-5 saving model... └ @ Main In[9]:41 . Epoch 63/200 train loss: 1.6e-5 validate loss: 1.5e-5 Epoch 64/200 train loss: 1.5e-5 validate loss: 1.5e-5 Epoch 65/200 train loss: 1.5e-5 validate loss: 1.5e-5 Epoch 66/200 train loss: 1.5e-5 validate loss: 1.5e-5 . ┌ Info: new best accuracy 1.4e-5 saving model... └ @ Main In[9]:41 . Epoch 67/200 train loss: 1.5e-5 validate loss: 1.4e-5 Epoch 68/200 train loss: 1.4e-5 validate loss: 1.4e-5 Epoch 69/200 train loss: 1.4e-5 validate loss: 1.4e-5 Epoch 70/200 train loss: 1.4e-5 validate loss: 1.4e-5 . ┌ Info: new best accuracy 1.3e-5 saving model... └ @ Main In[9]:41 . Epoch 71/200 train loss: 1.4e-5 validate loss: 1.3e-5 Epoch 72/200 train loss: 1.4e-5 validate loss: 1.3e-5 Epoch 73/200 train loss: 1.3e-5 validate loss: 1.3e-5 Epoch 74/200 train loss: 1.3e-5 validate loss: 1.3e-5 Epoch 75/200 train loss: 1.3e-5 validate loss: 1.3e-5 . ┌ Info: new best accuracy 1.2e-5 saving model... └ @ Main In[9]:41 . Epoch 76/200 train loss: 1.3e-5 validate loss: 1.2e-5 Epoch 77/200 train loss: 1.2e-5 validate loss: 1.2e-5 Epoch 78/200 train loss: 1.2e-5 validate loss: 1.2e-5 Epoch 79/200 train loss: 1.2e-5 validate loss: 1.2e-5 Epoch 80/200 train loss: 1.2e-5 validate loss: 1.2e-5 . ┌ Info: new best accuracy 1.1e-5 saving model... └ @ Main In[9]:41 . Epoch 81/200 train loss: 1.2e-5 validate loss: 1.1e-5 Epoch 82/200 train loss: 1.1e-5 validate loss: 1.1e-5 Epoch 83/200 train loss: 1.1e-5 validate loss: 1.1e-5 Epoch 84/200 train loss: 1.1e-5 validate loss: 1.1e-5 Epoch 85/200 train loss: 1.1e-5 validate loss: 1.1e-5 Epoch 86/200 train loss: 1.1e-5 validate loss: 1.1e-5 Epoch 87/200 train loss: 1.1e-5 validate loss: 1.1e-5 Epoch 88/200 train loss: 1.1e-5 validate loss: 1.1e-5 Epoch 89/200 train loss: 1.1e-5 validate loss: 1.1e-5 . ┌ Info: Not improved in 5 epochs. Dropping learning rate to 2.5e-5 └ @ Main In[9]:46 ┌ Info: new best accuracy 1.0e-5 saving model... └ @ Main In[9]:41 . Epoch 90/200 train loss: 1.0e-5 validate loss: 1.0e-5 Epoch 91/200 train loss: 1.0e-5 validate loss: 1.0e-5 Epoch 92/200 train loss: 1.0e-5 validate loss: 1.0e-5 Epoch 93/200 train loss: 1.0e-5 validate loss: 1.0e-5 Epoch 94/200 train loss: 1.0e-5 validate loss: 1.0e-5 Epoch 95/200 train loss: 1.0e-5 validate loss: 1.0e-5 Epoch 96/200 train loss: 1.0e-5 validate loss: 1.0e-5 . ┌ Info: new best accuracy 9.0e-6 saving model... └ @ Main In[9]:41 . Epoch 97/200 train loss: 1.0e-5 validate loss: 9.0e-6 Epoch 98/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 99/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 100/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 101/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 102/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 103/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 104/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 105/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 106/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 107/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 108/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 109/200 train loss: 9.0e-6 validate loss: 9.0e-6 . ┌ Info: Not improved in 5 epochs. Dropping learning rate to 1.25e-5 └ @ Main In[9]:46 . Epoch 110/200 train loss: 9.0e-6 validate loss: 9.0e-6 Epoch 111/200 train loss: 9.0e-6 validate loss: 9.0e-6 . ┌ Info: new best accuracy 8.0e-6 saving model... └ @ Main In[9]:41 . Epoch 112/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 113/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 114/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 115/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 116/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 117/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 118/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 119/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 120/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 121/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 122/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 123/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 124/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 125/200 train loss: 8.0e-6 validate loss: 8.0e-6 Epoch 126/200 train loss: 8.0e-6 validate loss: 8.0e-6 . ┌ Info: Not improved in 10 epochs. Converged I guess └ @ Main In[9]:51 . test_data = rand(test) create_gif_from_raw(test_data) . ┌ Info: Saved animation to │ fn = /notebooks/anim_fps30.gif └ @ Plots /opt/julia/packages/Plots/LSKOd/src/animation.jl:114 . input = Flux.unsqueeze(test_data&#39;, 3) output = new_model(input) output = reshape(output, 60,900)&#39; create_gif_from_raw(output) . ┌ Info: Saved animation to │ fn = /notebooks/anim_fps30.gif └ @ Plots /opt/julia/packages/Plots/LSKOd/src/animation.jl:114 . Results . After a few hours of training on GPU, we can now reasonably encode the movement of the whole swarm over 60 timesteps into 100 variables. However, I want to reduce that encoding even further into ~10 parameters that can be used to sonify the dynamics. . However, more pressingly I need the latent space to be regularised, and therefore I will adapt this model to be a variational auto-encoder. . Usually VAE&#39;s are useful if you intend on using them as generative processes, being able to sample from the latent space without worring about overfitting i.e. that some points in the latent space are meaningless once decoded. . My desire for a regularised latent space comes from the fact that I want differences in the latent space, to correspond with differences in the decoded output since I want to use the latent space variables as control parameters for sonification. .",
            "url": "https://maxworgan.github.io/blog/dimension%20reduction/auto-encoder/swarm/2022/02/06/DCAE_Part_2.html",
            "relUrl": "/dimension%20reduction/auto-encoder/swarm/2022/02/06/DCAE_Part_2.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Convolutional Auto Encoders - Part 1",
            "content": "Introduction . My aim is to try and encode a simulated swarm into a lower dimentional representation so I can identify interesting parameters with which to map into a sound generating system. . My initial research into reducing complex time-series data lead me to a paper by Ali et al . They use convolutional layers in an auto encoder to do dimension reduction on various time series data. . This seemed like a good place to start, so initially I am just documenting my attempt to recreate their research, using a single agents coordinates in 3d space to match the shape of their model. . Details . Their approach uses 1D Convolution and the data split into interleaved sliding windows of data: . They used a 60 frame window and 3 readings from accelerometers resulting in a 60x3 shape input image. . As a test of their approach I used a single agents movement data to match the original model specifications. . Data is captured from a separate flocking simulation (source available on my github soon) of 300 agents over approximately 30 seconds. . using Flux, CSV, DataFrames, MLDataPattern, CUDA, Plots, WebIO; plotly() . Resolving package versions... No Changes to `~/Documents/JuliaProjects/Swarm/Project.toml` No Changes to `~/Documents/JuliaProjects/Swarm/Manifest.toml` . Plots.PlotlyBackend() . df = DataFrame(CSV.File(&quot;$(datadir())/exp_raw/data.csv&quot;; types=Float32)) df = df[:,1:3] plot(df[:,1],df[:,2], df[:,3]) . We can treat this 3D data as 3 separate 1D data points, matching the original TimeCluster data shape . plot(df[:,1], label=&quot;x&quot;) plot!(df[:,2], label=&quot;y&quot;) plot!(df[:,3], label=&quot;z&quot;) . Data Noramlisation and preperation . Normalise the data into the range [0, 1] as per the paper. We then create a sliding window using the defaults from the paper where stride = 1 and window_size = 60 . function normalise(M) min = minimum(minimum(eachcol(M))) max = maximum(maximum(eachcol(M))) return (M .- min) ./ (max - min) end normalised = Array(df) |&gt; normalise window_size = 60 data = slidingwindow(normalised&#39;,window_size,stride=1); . Define the encoder and decoder . We create the auto-encoder network as per the paper . function create_ae() # Define the encoder and decoder networks encoder = Chain( # 60x3xb Conv((10,), 3 =&gt; 64, relu; pad = SamePad()), MaxPool((2,)), # 30x64xb Conv((5,), 64 =&gt; 32, relu; pad = SamePad()), MaxPool((2,)), # 15x32xb Conv((5,), 32 =&gt; 12, relu; pad = SamePad()), MaxPool((3,)), # 5x12xb Flux.flatten, Dense(window_size,window_size) ) decoder = Chain( # input 60 (x -&gt; reshape(x, (floor(Int, (window_size / 12)),12,:))), # 5x12xb ConvTranspose((5,), 12 =&gt; 32, relu; pad = SamePad()), Upsample((3,)), # 15x32xb ConvTranspose((5,), 32 =&gt; 64, relu; pad = SamePad()), Upsample((2,)), # 30x64xb ConvTranspose((10,), 64 =&gt; 3, relu; pad = SamePad()), Upsample((2,)), # 60x3xb ) return Chain(encoder, decoder) end . create_ae (generic function with 1 method) . Training . In keeping with the paper we use the Mean Square Error loss function and the ADAM optimiser . function train_model!(model, data, opt; epochs=20, bs=16, dev=Flux.gpu) model = model |&gt; dev ps = params(model) t = shuffleobs(data) local l losses = Vector{Float64}() for e in 1:epochs for x in eachbatch(t, size=bs) # bs[(3, 60)] x = cat(x..., dims=3) # bs x 3 x 60 x = permutedims(x, [2,1,3]) # 60 x 3 x bs gs = gradient(ps) do l = loss(model(x),x) end Flux.update!(opt, ps, gs) end l = round(l;digits=6) push!(losses, l) println(&quot;Epoch $e/$epochs - train loss: $l&quot;) end model = model |&gt; cpu; losses end . train_model! (generic function with 1 method) . loss(x,y) = Flux.Losses.mse(x, y) opt = Flux.Optimise.ADAM(0.00005) epochs = 100 . loss (generic function with 1 method) . model = create_ae() losses_01 = train_model!(model, data, opt; epochs=epochs); . Epoch 1/100 - train loss: 0.018689 Epoch 2/100 - train loss: 0.004161 Epoch 3/100 - train loss: 0.002182 Epoch 4/100 - train loss: 0.001568 Epoch 5/100 - train loss: 0.001264 Epoch 6/100 - train loss: 0.001071 Epoch 7/100 - train loss: 0.000899 Epoch 8/100 - train loss: 0.000703 Epoch 9/100 - train loss: 0.0005 Epoch 10/100 - train loss: 0.000352 Epoch 11/100 - train loss: 0.00028 Epoch 12/100 - train loss: 0.000236 Epoch 13/100 - train loss: 0.000202 Epoch 14/100 - train loss: 0.000178 Epoch 15/100 - train loss: 0.000159 Epoch 16/100 - train loss: 0.000142 Epoch 17/100 - train loss: 0.000127 Epoch 18/100 - train loss: 0.000113 Epoch 19/100 - train loss: 0.000101 Epoch 20/100 - train loss: 9.2e-5 Epoch 21/100 - train loss: 8.3e-5 Epoch 22/100 - train loss: 7.5e-5 Epoch 23/100 - train loss: 6.8e-5 Epoch 24/100 - train loss: 6.3e-5 Epoch 25/100 - train loss: 5.7e-5 Epoch 26/100 - train loss: 5.2e-5 Epoch 27/100 - train loss: 4.8e-5 Epoch 28/100 - train loss: 4.4e-5 Epoch 29/100 - train loss: 4.1e-5 Epoch 30/100 - train loss: 3.8e-5 Epoch 31/100 - train loss: 3.6e-5 Epoch 32/100 - train loss: 3.4e-5 Epoch 33/100 - train loss: 3.2e-5 Epoch 34/100 - train loss: 3.0e-5 Epoch 35/100 - train loss: 2.9e-5 Epoch 36/100 - train loss: 2.7e-5 Epoch 37/100 - train loss: 2.6e-5 Epoch 38/100 - train loss: 2.4e-5 Epoch 39/100 - train loss: 2.3e-5 Epoch 40/100 - train loss: 2.3e-5 Epoch 41/100 - train loss: 2.2e-5 Epoch 42/100 - train loss: 2.1e-5 Epoch 43/100 - train loss: 2.1e-5 Epoch 44/100 - train loss: 2.1e-5 Epoch 45/100 - train loss: 2.1e-5 Epoch 46/100 - train loss: 2.0e-5 Epoch 47/100 - train loss: 2.0e-5 Epoch 48/100 - train loss: 2.0e-5 Epoch 49/100 - train loss: 1.9e-5 Epoch 50/100 - train loss: 1.9e-5 Epoch 51/100 - train loss: 1.9e-5 Epoch 52/100 - train loss: 1.9e-5 Epoch 53/100 - train loss: 1.9e-5 Epoch 54/100 - train loss: 1.9e-5 Epoch 55/100 - train loss: 1.9e-5 Epoch 56/100 - train loss: 1.8e-5 Epoch 57/100 - train loss: 1.8e-5 Epoch 58/100 - train loss: 1.7e-5 Epoch 59/100 - train loss: 1.6e-5 Epoch 60/100 - train loss: 1.5e-5 Epoch 61/100 - train loss: 1.5e-5 Epoch 62/100 - train loss: 1.4e-5 Epoch 63/100 - train loss: 1.3e-5 Epoch 64/100 - train loss: 1.3e-5 Epoch 65/100 - train loss: 1.3e-5 Epoch 66/100 - train loss: 1.3e-5 Epoch 67/100 - train loss: 1.2e-5 Epoch 68/100 - train loss: 1.2e-5 Epoch 69/100 - train loss: 1.2e-5 Epoch 70/100 - train loss: 1.2e-5 Epoch 71/100 - train loss: 1.2e-5 Epoch 72/100 - train loss: 1.2e-5 Epoch 73/100 - train loss: 1.1e-5 Epoch 74/100 - train loss: 1.1e-5 Epoch 75/100 - train loss: 1.1e-5 Epoch 76/100 - train loss: 1.1e-5 Epoch 77/100 - train loss: 1.1e-5 Epoch 78/100 - train loss: 1.0e-5 Epoch 79/100 - train loss: 1.0e-5 Epoch 80/100 - train loss: 1.0e-5 Epoch 81/100 - train loss: 1.0e-5 Epoch 82/100 - train loss: 1.0e-5 Epoch 83/100 - train loss: 1.0e-5 Epoch 84/100 - train loss: 1.0e-5 Epoch 85/100 - train loss: 1.0e-5 Epoch 86/100 - train loss: 1.0e-5 Epoch 87/100 - train loss: 1.0e-5 Epoch 88/100 - train loss: 1.0e-5 Epoch 89/100 - train loss: 1.0e-5 Epoch 90/100 - train loss: 1.0e-5 Epoch 91/100 - train loss: 1.0e-5 Epoch 92/100 - train loss: 1.0e-5 Epoch 93/100 - train loss: 1.0e-5 Epoch 94/100 - train loss: 1.0e-5 Epoch 95/100 - train loss: 1.0e-5 Epoch 96/100 - train loss: 1.0e-5 Epoch 97/100 - train loss: 1.0e-5 Epoch 98/100 - train loss: 1.0e-5 Epoch 99/100 - train loss: 1.0e-5 Epoch 100/100 - train loss: 1.0e-5 . . plot(losses_01, label=&quot;&quot;) xlabel!(&quot;Epochs&quot;) ylabel!(&quot;Mean Squared Error&quot;) . Lets see how well it&#39;s able to reconstruct a random segment of the data . input = rand(data)&#39; plot(input[:,1],input[:, 2], input[:,3], label=&quot;original&quot;) # Plot the reconstructed data in red output = model(Flux.unsqueeze(input, 3)) plot!(output[:,1], output[:, 2], output[:,3], label=&quot;reconstructed&quot;) . ┌ Info: loss: │ loss(input, output) = 6.2904514e-6 └ @ Main In[72]:6 . @info &quot;loss:&quot; loss(input, output) . ┌ Info: loss: │ loss(input, output) = 6.2904514e-6 └ @ Main In[74]:2 . Conclusion . This approach clearly is able to reconstruct the input data from a low dimensional representation. Before we go further (and use clustering on the latent space to identify parameters), lets try and scale this up to a 300 agent swarm and see what changes we need to make. .",
            "url": "https://maxworgan.github.io/blog/dimension%20reduction/auto-encoder/swarm/2022/01/14/DCAE-Part-1.html",
            "relUrl": "/dimension%20reduction/auto-encoder/swarm/2022/01/14/DCAE-Part-1.html",
            "date": " • Jan 14, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": "Max Worgan is a PhD candidate at Sussex University, UK. . His research interests include, computer music, machine learning, and complex dynamical systems. .",
          "url": "https://maxworgan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://maxworgan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}