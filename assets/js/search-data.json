{
  
    
        "post0": {
            "title": "DCAE",
            "content": "Start . My aim is to try and encode a simulated swarm into a lower dimentional representation so I can identify interesting parameters with which to map into a sound generating system. . My initial research into reducing complex time-series data lead me to a paper by Ali et al . They use convolutional layers in an auto encoder to do dimension reduction on various time series data. . This seemed like a good place to start, so initially I am just documenting my attempt to recreate their research, using a single agents coordinates in 3d space to match the shape of their experimental data. . Details . Their approach uses 1D Convolution and the data split into interleaved sliding windows of data: . They used a 60 frame window and 3 readings from accelerometers resulting in a 60x3 shape input image. . As a test of their approach I used a single agents movement data to match the original model specifications. . Data is captured from a separate flocking simulation (source available on my github soon) of 300 agents over approximately 30 seconds. . df = DataFrame(CSV.File(&quot;$(datadir())/exp_raw/data.csv&quot;; header=[&quot;x&quot;,&quot;y&quot;,&quot;z&quot;], types=Float32)) df = df[:,1:3] # just use the first 3 cols - i.e. x,y,z of first agent @df df StatsPlots.plot(cols(1),cols(2), cols(3)) . ┌ Warning: thread = 1 warning: parsed expected 3 columns, but didn&#39;t reach end of line around data row: 1. Parsing extra columns and widening final columnset └ @ CSV /Users/max/.julia/packages/CSV/nofYz/src/file.jl:635 . We can treat this 3D data as 3 separate 1D data points, matching the original TimeCluster data shape . @df df StatsPlots.plot(cols(1:3)) . Data Noramlisation and preperation . Normalise the data into the range [0, 1] as per the paper. We then create a sliding window using the defaults from the paper where stride = 1 and window_size = 60 . function normalise(M) min = minimum(minimum(eachcol(M))) max = maximum(maximum(eachcol(M))) return (M .- min) ./ (max - min) end normalised = Array(df) |&gt; normalise window_size = 60 data = slidingwindow(normalised&#39;,window_size,stride=1); . Define the encoder and decoder . We create the auto-encoder network as per the paper . function create_ae() # Define the encoder and decoder networks encoder = Chain( # 60x3xb Conv((10,), 3 =&gt; 64, relu; pad = SamePad()), MaxPool((2,)), # 30x64xb Conv((5,), 64 =&gt; 32, relu; pad = SamePad()), MaxPool((2,)), # 15x32xb Conv((5,), 32 =&gt; 12, relu; pad = SamePad()), MaxPool((3,)), # 5x12xb Flux.flatten, Dense(window_size,window_size) ) decoder = Chain( # input 60 (x -&gt; reshape(x, (floor(Int, (window_size / 12)),12,:))), # 5x12xb ConvTranspose((5,), 12 =&gt; 32, relu; pad = SamePad()), Upsample((3,)), # 15x32xb ConvTranspose((5,), 32 =&gt; 64, relu; pad = SamePad()), Upsample((2,)), # 30x64xb ConvTranspose((10,), 64 =&gt; 3, relu; pad = SamePad()), Upsample((2,)), # 60x3xb ) return Chain(encoder, decoder) end . create_ae (generic function with 1 method) . Training . In keeping with the paper we use the Mean Square Error loss function and the ADAM optimiser . function train_model!(model, data, opt; epochs=20, bs=16, dev=Flux.gpu) model = model |&gt; dev ps = params(model) t = shuffleobs(data) local l losses = Vector{Float64}() for e in 1:epochs for x in eachbatch(t, size=bs) # bs[(3, 60)] x = cat(x..., dims=3) # bs x 3 x 60 x = permutedims(x, [2,1,3]) # 60 x 3 x bs gs = gradient(ps) do l = loss(model(x),x) end Flux.update!(opt, ps, gs) end l = round(l;digits=6) push!(losses, l) println(&quot;Epoch $e/$epochs - train loss: $l&quot;) end model = model |&gt; cpu; losses end . train_model! (generic function with 1 method) . loss(x,y) = Flux.Losses.mse(x, y) . loss (generic function with 1 method) . model = create_ae() losses_01 = train_model!(model, data, Flux.Optimise.ADAM(0.00005); epochs=100); . Epoch 1/100 - train loss: 0.016247 Epoch 2/100 - train loss: 0.002978 Epoch 3/100 - train loss: 0.001985 Epoch 4/100 - train loss: 0.001512 Epoch 5/100 - train loss: 0.001201 Epoch 6/100 - train loss: 0.000987 Epoch 7/100 - train loss: 0.000851 Epoch 8/100 - train loss: 0.000746 Epoch 9/100 - train loss: 0.000633 Epoch 10/100 - train loss: 0.000531 Epoch 11/100 - train loss: 0.000427 Epoch 12/100 - train loss: 0.000332 Epoch 13/100 - train loss: 0.000265 Epoch 14/100 - train loss: 0.000221 Epoch 15/100 - train loss: 0.00019 Epoch 16/100 - train loss: 0.000166 Epoch 17/100 - train loss: 0.000148 Epoch 18/100 - train loss: 0.000131 Epoch 19/100 - train loss: 0.000116 Epoch 20/100 - train loss: 0.000104 Epoch 21/100 - train loss: 9.5e-5 Epoch 22/100 - train loss: 8.6e-5 Epoch 23/100 - train loss: 7.9e-5 Epoch 24/100 - train loss: 7.3e-5 Epoch 25/100 - train loss: 6.8e-5 Epoch 26/100 - train loss: 6.4e-5 Epoch 27/100 - train loss: 5.9e-5 Epoch 28/100 - train loss: 5.5e-5 Epoch 29/100 - train loss: 5.0e-5 Epoch 30/100 - train loss: 4.7e-5 Epoch 31/100 - train loss: 4.4e-5 Epoch 32/100 - train loss: 4.0e-5 Epoch 33/100 - train loss: 3.8e-5 Epoch 34/100 - train loss: 3.6e-5 Epoch 35/100 - train loss: 3.4e-5 Epoch 36/100 - train loss: 3.2e-5 Epoch 37/100 - train loss: 3.0e-5 Epoch 38/100 - train loss: 2.9e-5 Epoch 39/100 - train loss: 2.7e-5 Epoch 40/100 - train loss: 2.6e-5 Epoch 41/100 - train loss: 2.5e-5 Epoch 42/100 - train loss: 2.4e-5 Epoch 43/100 - train loss: 2.3e-5 Epoch 44/100 - train loss: 2.2e-5 Epoch 45/100 - train loss: 2.1e-5 Epoch 46/100 - train loss: 2.0e-5 Epoch 47/100 - train loss: 2.0e-5 Epoch 48/100 - train loss: 1.9e-5 Epoch 49/100 - train loss: 1.8e-5 Epoch 50/100 - train loss: 1.8e-5 Epoch 51/100 - train loss: 1.7e-5 Epoch 52/100 - train loss: 1.6e-5 Epoch 53/100 - train loss: 1.6e-5 Epoch 54/100 - train loss: 1.6e-5 Epoch 55/100 - train loss: 1.5e-5 Epoch 56/100 - train loss: 1.5e-5 Epoch 57/100 - train loss: 1.4e-5 Epoch 58/100 - train loss: 1.4e-5 Epoch 59/100 - train loss: 1.4e-5 Epoch 60/100 - train loss: 1.3e-5 Epoch 61/100 - train loss: 1.3e-5 Epoch 62/100 - train loss: 1.3e-5 Epoch 63/100 - train loss: 1.2e-5 Epoch 64/100 - train loss: 1.2e-5 Epoch 65/100 - train loss: 1.2e-5 Epoch 66/100 - train loss: 1.2e-5 Epoch 67/100 - train loss: 1.2e-5 Epoch 68/100 - train loss: 1.1e-5 Epoch 69/100 - train loss: 1.1e-5 Epoch 70/100 - train loss: 1.1e-5 Epoch 71/100 - train loss: 1.1e-5 Epoch 72/100 - train loss: 1.1e-5 Epoch 73/100 - train loss: 1.0e-5 Epoch 74/100 - train loss: 1.0e-5 Epoch 75/100 - train loss: 1.0e-5 Epoch 76/100 - train loss: 1.0e-5 Epoch 77/100 - train loss: 1.0e-5 Epoch 78/100 - train loss: 9.0e-6 Epoch 79/100 - train loss: 9.0e-6 Epoch 80/100 - train loss: 9.0e-6 Epoch 81/100 - train loss: 9.0e-6 Epoch 82/100 - train loss: 9.0e-6 Epoch 83/100 - train loss: 9.0e-6 Epoch 84/100 - train loss: 9.0e-6 Epoch 85/100 - train loss: 9.0e-6 Epoch 86/100 - train loss: 8.0e-6 Epoch 87/100 - train loss: 8.0e-6 Epoch 88/100 - train loss: 8.0e-6 Epoch 89/100 - train loss: 8.0e-6 Epoch 90/100 - train loss: 8.0e-6 Epoch 91/100 - train loss: 8.0e-6 Epoch 92/100 - train loss: 8.0e-6 Epoch 93/100 - train loss: 8.0e-6 Epoch 94/100 - train loss: 8.0e-6 Epoch 95/100 - train loss: 8.0e-6 Epoch 96/100 - train loss: 8.0e-6 Epoch 97/100 - train loss: 7.0e-6 Epoch 98/100 - train loss: 7.0e-6 Epoch 99/100 - train loss: 7.0e-6 Epoch 100/100 - train loss: 7.0e-6 . . PlotlyJS.plot( PlotlyJS.scatter(y=losses_01, x=1:80, mode=&quot;lines+markers&quot;) , Layout(xaxis_title=&quot;Epoch&quot;, yaxis_title=&quot;Mean Squared Error&quot;)) . Lets see how well it&#39;s able to reconstruct a random segment of the data . test = rand(data) test_df = DataFrame(test&#39;, :auto) @df test_df StatsPlots.plot(cols(1),cols(2), cols(3), label=&quot;input&quot;) # Plot the reconstructed data in red input = Flux.unsqueeze(test&#39;, 3) output = model(input) @info &quot;loss:&quot; loss(input, output) output_df = DataFrame(reshape(output, (window_size,3)), :auto) @df output_df StatsPlots.plot!(cols(1),cols(2), cols(3), label=&quot;reconstructed&quot;) . ┌ Info: loss: │ loss(input, output) = 5.021019e-6 └ @ Main In[54]:9 .",
            "url": "https://maxworgan.github.io/blog/dimension%20reduction/auto-encoder/swarm/2022/01/21/DCAE.html",
            "relUrl": "/dimension%20reduction/auto-encoder/swarm/2022/01/21/DCAE.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://maxworgan.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": "Max Worgan is a PhD candidate at Sussex University, UK. . His research interests include, computer music, machine learning, and complex dynamical systems. .",
          "url": "https://maxworgan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://maxworgan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}